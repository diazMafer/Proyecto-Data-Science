{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deteccion de edad ósea - Proyecto 2\n",
    "\n",
    "Alexander Trujillo - 17189\n",
    "\n",
    "Javier Carpio - 17077\n",
    "\n",
    "Francisco Molina - 17050\n",
    "\n",
    "Ana Lucia Hernandez - 17138\n",
    "\n",
    "Andrea Arguello - 17501\n",
    "\n",
    "Maria Fernanda Lopez - 17160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Situación problemática\n",
    "Por décadas, la determinación de la madurez ósea se ha basado en una evaluación visual del desarrollo esquelético de la mano y la muñeca (Gilsanz y Ratib, 2005). Por lo tanto, sería útil el determinar esta de manera automatizada. Para esto, se cuenta con un conjunto de datos de entrenamiento con 12.6k imágenes de radiografías de infantes, y se provee además su género y su edad, esta última siendo la variable dependiente, ya que el objetivo es, a partir de la radiografía (o la misma y el género), determinar de forma automatizada la edad ósea de los infantes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema científico\n",
    "Conocer la edad ósea (EO) de un individuo es necesaria para confirmar el diagnóstico de variantes normales del crecimiento, así como para orientar el diagnóstico de condiciones patológicas, tales como diagnosticar y tratar una multitud de trastornos endocrinos y síndromes pediátricos, para decidir el inicio o el cese de ciertos tratamientos y para estimar la talla adulta, y para estimar la edad de niños con fecha de nacimiento desconocida. Sin embargo, durante décadas, la determinación de la madurez ósea se ha basado en una evaluación visual del desarrollo esquelético de la mano y la muñeca (Gilsanz y Ratib, 2005; Navarro, Tejedor y López Siguero, 2014). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "### General\n",
    "Aplicar los conceptos de reconocimiento de imágenes vistos en clase, tanto la limpieza y procesamiento de las mismas como la creación de redes neuronales convolucionales (CNN por sus siglas en inglés), y aplicarlos a situaciones de la vida real (en este caso, en el ámbito de la pediatría).\n",
    "\n",
    "### Específicos\n",
    "1. Predecir la edad ósea del paciente, con base en las imágenes provistas de radiografías de manos, con una precisión de $\\pm$4.2 meses.\n",
    "2. Evaluar si incluir el género como variable tiene correlación con la precisión de la edad calculada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción de los datos\n",
    "Los datos a trabajar son los de *RSNA Bone Age* los cuales contienen imagenes de rayos x digitalizadas y escaneadas de la mano de una persona, también se cuenta con un csv en el cual se indica la edad real y genero de dicha imagen, esta edad es la que se predecira con el modelo a elobarar.\n",
    "\n",
    "Se cuenta con datos de test y de training, cada uno con su respectiva cantidad de imágenes y su propio csv. Como se menciono anteriormente dichos csv contienen dos datos importantes la edad y el genero. Al observar los csv de train y de test podemos observar que la columna en donde se indica el genero no es llamada igual y no se utilizan los mismos datos para describir el sexo. \n",
    "En el caso del csv de train podemos observar que la columna para describir el sexo de la persona se llama 'Male' y sus datos son tipo booleano teniendo un 'True' para cuando es hombre y un 'False' para cuando se es mujer. A diferencia de test que podemos observar que esta columna sí se llama 'Sex' y cuenta con dos valores tipo string 'M' para indicar  hombre ('Male' en ingles) y 'F' para indicar mujer ('Female' en ingles). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/rsna-bone-age/boneage-training-dataset.csv')\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de datos \n",
    "\n",
    "Se cambia variable male para que este este en los términos del test dataset y mantener coherencia entre ambos datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpieza \n",
    "train = train.assign(sex = lambda train: train['male'].map(lambda male: \"M\" if male == True else \"F\") )\n",
    "train = train.drop(['male'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['sex'].value_counts()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "langs = ['Masculino',\"Femenino\"]\n",
    "ax.bar(langs,y)\n",
    "plt.title('Cantidad de hombres y mujeres')\n",
    "plt.ylabel('Cantidad')\n",
    "plt.xlabel('Género')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = ['M','F']\n",
    "explode = (0.2, 0, 0, 0, 0, 0)\n",
    "plt.pie(train['sex'].value_counts(), labels=labels,\n",
    "autopct='%1.1f%%', shadow=True)\n",
    "plt.title('Population Density Index')\n",
    "\n",
    "plt.title('Population Density Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que los datos están casi equitativamente distribuidos entre hombres y mujeres. Es por ello que no se considera necesario aplicar balanceo de clases de los datos proporcionados por el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.hist('boneage', bins = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que los datos están principalmente concentrados entre 125  y 175 meses, el histograma presenta sesgo a la izquierda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['boneage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.boxplot('boneage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.boxplot('boneage', by='sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_id, boneage, sex in train[['id','boneage', 'sex']].sample(5).values:\n",
    "    img_name = str(img_id) + '.png'\n",
    "    img = mpimg.imread(\"../input/rsna-bone-age/boneage-training-dataset/boneage-training-dataset/\"+img_name)\n",
    "    plt.imshow(img)\n",
    "    plt.title('Image: {} Boneage: {} Sex: {}'.format(img_name, boneage, sex))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar en las imágenes que todas las radiografías son de la mano izquierda, esto puede ser al método con el cual se calculó la edad osea. De igual manera se puede ver que la calidad de las radiografías no es igual para todas como se observa en la cuarta imagen que los huesos no se aprecian de igual forma a comparación de las demás en donde estos se ven de un color verde fosforescente.\n",
    "\n",
    "Al haber imágenes con esta calidad podemos asumir que para la predicción se tendrá que hacer una transformación en la imagen en donde se resalten estos huesos en alguna escala de grises o bien con alguna textura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_id, boneage, sex in train[['id','boneage', 'sex']].sample(5).values:\n",
    "    img_name = str(img_id) + '.png'\n",
    "    im = Image.open(\"../input/rsna-bone-age/boneage-training-dataset/boneage-training-dataset/\"+img_name)\n",
    "    width, height = im.size\n",
    "    print(width,height) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar además que las imágenes vienen en distintos tamaños, y el tamaño de las mismas es bastante grande."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hallazgos y conclusiones \n",
    "\n",
    "Debido a que el train set contiene la variable de género como un booleano indicando si es hombre o mujer, fue necesario modificar dicha columna, pues en el test set se encontraba como un string indicando 'M' y 'F' para los mismos.\n",
    "\n",
    "El dataset contiene una mayor cantidad de observaciones de hombres, sin embargo, la proporción de los mismos es de aproximadamente 54% hombres y 46% mujeres. Debido a que la diferencia no es significativa, no se considera necesario el hacer un aumento de las entradas para emparejar los datos. \n",
    "\n",
    "Se puede observar que la edad está dada en meses, con un sesgo a la izquierda, indicando que la mayoría de los datos, al igual que lo que refleja el diagrama de caja y bigote, se encuentran por debajo de los 132 meses (es decir, 11 años). Además, los hombres presentan edades más altas en el dataset, con una mediana de 150, mientras que la mediana de las mujeres se encuentra por debajo de la del dataset completo, aproximadamente por 124-125 meses. Se puede notar, en el diagrama de caja y bigote de todo el dataset, que hay datos atípicos en las edades más bajas, por lo cual el modelo podría tener problemas a la hora de predecir la edad ósea dentro del rango deseado.\n",
    "\n",
    "Ya que todas las imágenes son de la mano izquierda, no es necesario eliminar fotografías en el dataset, ya que de haber alguna que fuese de la mano derecha, esto podría afectar a la hora de realizar el modelo. \n",
    "\n",
    "## Conclusiones sobre los siguientes pasos a seguir\n",
    "\n",
    "Algunas imágenes se encuentran muy \"oscuras\", refiriéndose a que los huesos no se distinguen claramente. Será necesario recuperar la información, ya sea cambiando los colores a blanco y negro u obteniendo las derivadas de las mismas para trabajarlas con las texturas.\n",
    "\n",
    "Será necesario realizar transformaciones sobre las fotos además, debido a las altas dimensiones de las mismas y a que no hay un \"estándar\" en el conjunto de datos, ya que hay distintos tamaños y dimensiones.\n",
    "\n",
    "Se planea trabajar con una red neuronal convolucional, debido a que se está trabajando con imágenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmos de aprendizaje útiles para abordar el problema\n",
    "### Artificial Neural Network\n",
    "Con una red de este tipo se pueden realizar análisis multifactoriales. Consiste en una estructura multicapa que contiene nodos conectados por bordes ponderados que establecen una capa de entrada, una o más capas ocultas y una capa de salida. Con valores de entrada y salida conocidos, la red se entrena ajustando los pesos de forma incremental hasta que la salida de la red se aproxima a la salida conocida. Esta red predice de forma eficiente, por lo general es aplicada a la interpretación de imágenes, pero al contrario que en el caso de regresión, se trata de una técnica en donde los resultados no pueden interpretarse de forma intuitiva (Dallora, Anderberg, Kvist, Mendes, Diaz Ruiz, y Berglund, 2019).\n",
    "\n",
    "### Convolutional Neural Network\n",
    "Un modelo de clasificación de imágenes de CNN toma una imagen de entrada, la pasa a través de una serie de capas convolucionales, no lineales, agrupadas y completamente conectadas, y finalmente nos da una salida. Una imagen de entrada es básicamente una matriz de píxeles en forma de matriz: h x w x d (h = altura, w = ancho, d = dimensión). Cada imagen de entrada pasará a través de una serie de capas de convolución con filtros. Y en la práctica, una CNN aprende los valores de estos filtros por sí misma durante el proceso de formación. Este filtro se llama \"kernel\" y también es una matriz de números. Imaginemos que este kernel se desliza por todas las áreas de la imagen de entrada con un ancho de paso predefinido. Este ancho de paso o la distancia que se mueve un kernel cada vez que pasa sobre una imagen se define como un \"paso\" (Husseini, Spronk, Masanneck, y Gutewort, 2019).\n",
    "\n",
    "### Bayesian Network\n",
    "La red bayesiana estima la probabilidad posterior de que un punto de datos sea de una determinada clase, dado un conjunto de características. El proceso de aprendizaje ocurre en dos fases: primero aprende la estructura de la red, que es un gráfico acíclico directo compuesto por nodos (que representan las características) y aristas (que representan las dependencias probabilísticas), luego aprende la distribución de probabilidad condicional de cada nodo. Las ventajas de las redes bayesianas son que pueden codificar el conocimiento de los expertos en el dominio en la estructura del gráfico y pueden trabajar con cantidades más pequeñas de datos. Una gran desventaja es que las redes bayesianas pueden volverse poco prácticas en problemas con un gran número de variables (Dallora, Anderberg, Kvist, Mendes, Diaz Ruiz, y Berglund, 2019).\n",
    "\n",
    "## Algoritmo a utilizar\n",
    "Se utilizarán modelos de una red convolucional, el motivo de haber elegido una red convolucional es debido a su manejo de los pixeles y cuadros de la imagen que puede tener debido a la arquitectura que esta tiene que fue explicada anteriormente, y ya que, según el laboratorio realizado en clase, es mejor que una ANN para los problemas de análisis de imágenes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura y preparación de imagenes\n",
    "\n",
    "Método obtenido de https://www.kaggle.com/dansbecker/tensorflow-programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.3.1-cp38-cp38-macosx_10_14_x86_64.whl (165.2 MB)\n",
      "Requirement already satisfied: wheel>=0.26 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow) (0.35.1)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Using cached protobuf-3.13.0-cp38-cp38-macosx_10_9_x86_64.whl (1.3 MB)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Using cached tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "Collecting astunparse==1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Using cached absl_py-0.10.0-py3-none-any.whl (127 kB)\n",
      "Collecting tensorboard<3,>=2.3.0\n",
      "  Using cached tensorboard-2.3.0-py3-none-any.whl (6.8 MB)\n",
      "Collecting numpy<1.19.0,>=1.16.0\n",
      "  Using cached numpy-1.18.5-cp38-cp38-macosx_10_9_x86_64.whl (15.1 MB)\n",
      "Processing /Users/polaris/Library/Caches/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501/termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Using cached grpcio-1.32.0-cp38-cp38-macosx_10_9_x86_64.whl (3.3 MB)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Processing /Users/polaris/Library/Caches/pip/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73/wrapt-1.12.1-cp38-cp38-macosx_10_9_x86_64.whl\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Collecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from protobuf>=3.9.2->tensorflow) (47.1.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Using cached google_auth-1.22.0-py2.py3-none-any.whl (114 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.10)\n",
      "Collecting aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\"\n",
      "  Using cached aiohttp-3.6.2-py3-none-any.whl (441 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Using cached rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting multidict<5.0,>=4.5\n",
      "  Using cached multidict-4.7.6-cp38-cp38-macosx_10_14_x86_64.whl (48 kB)\n",
      "Collecting async-timeout<4.0,>=3.0\n",
      "  Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (19.3.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.6.0-cp38-cp38-macosx_10_14_x86_64.whl (128 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Installing collected packages: protobuf, tensorflow-estimator, astunparse, numpy, opt-einsum, absl-py, grpcio, markdown, werkzeug, oauthlib, requests-oauthlib, multidict, async-timeout, yarl, aiohttp, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard-plugin-wit, tensorboard, termcolor, google-pasta, gast, wrapt, keras-preprocessing, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.1\n",
      "    Uninstalling numpy-1.19.1:\n",
      "      Successfully uninstalled numpy-1.19.1\n",
      "Successfully installed absl-py-0.10.0 aiohttp-3.6.2 astunparse-1.6.3 async-timeout-3.0.1 cachetools-4.1.1 gast-0.3.3 google-auth-1.22.0 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.32.0 keras-preprocessing-1.1.2 markdown-3.2.2 multidict-4.7.6 numpy-1.18.5 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.13.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.6 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.1 tensorflow-estimator-2.3.0 termcolor-1.1.0 werkzeug-1.0.1 wrapt-1.12.1 yarl-1.6.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "# from learntools.deep_learning.decode_predictions import decode_predictions\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_prep_images(img_paths, img_height, img_width, color_mode=\"grayscale\"):\n",
    "    imgs = [load_img(img_path, target_size=(img_height, img_width, 1), color_mode=color_mode) for img_path in img_paths]\n",
    "    img_array = np.array([img_to_array(img) for img in imgs])\n",
    "    return(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtener todos los paths de los archivos \n",
    "\n",
    "base_path = '../input/rsna-bone-age/'\n",
    "\n",
    "train_imgs = [ [int(file.split(\".\")[0]), join(base_path + (\"boneage-training-dataset/\"*2), file)] for file in listdir(base_path + (\"boneage-training-dataset/\"*2)) ]\n",
    "# test_imgs = [ [int(file.split(\".\")[0]), join(base_path + (\"boneage-test-dataset/\"*2), file)] for file in listdir(base_path + (\"boneage-test-dataset/\"*2)) ]\n",
    "\n",
    "# obtener train y test dfs\n",
    "train_imgs_df = pd.DataFrame(train_imgs, columns = [\"id\", \"path\"])\n",
    "# test_imgs_df = pd.DataFrame(test_imgs, columns = [\"id\", \"path\"])\n",
    "\n",
    "train_csv = pd.read_csv('../input/rsna-bone-age/boneage-training-dataset.csv')\n",
    "# train_csv = train_csv.assign(sex = lambda train: train['male'].map(lambda male: \"M\" if male == True else \"F\") )\n",
    "# train_csv = train_csv.drop(['male'], axis=1) # No la droppeamos ya que más adelante queremos ver si nos ayuda tener el género\n",
    "\n",
    "# test_csv = pd.read_csv('../input/rsna-bone-age/boneage-test-dataset.csv').rename(columns={\"Case ID\": \"id\", \"Sex\":\"sex\"})\n",
    "\n",
    "data = train_imgs_df.merge(train_csv, on=\"id\", how=\"inner\")\n",
    "# test = test_imgs_df.merge(test_csv, on=\"id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se hará el split de la data, en 65% para training y 35% de testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = np.split(data.sample(frac=1), [int(.65*len(data))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguidamente, se realizará el procesamiento de imágenes tanto para el segmento de training como para testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 300\n",
    "#X_train = read_and_prep_images(train[\"path\"].values, img_height = img_size, img_width = img_size)\n",
    "y_train = train['boneage'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test = read_and_prep_images(test[\"path\"].values, img_height = img_size, img_width = img_size)\n",
    "y_test = test['boneage'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save de las matrices de train y test, solo de la matriz X ya que eso es lo que mas se tarda\n",
    "#with open('X_train_2.npy', 'wb') as f:\n",
    "#    np.save(f, X_train)\n",
    "#with open('X_test_2.npy', 'wb') as f:\n",
    "#    np.save(f, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load de las matrices\n",
    "# X_train = np.load(r'./X_train.npy')\n",
    "# X_test = np.load(r'./X_test.npy')\n",
    "with open('X_train.npy', 'rb') as f: #idk about this porque no sé si el X_train se guarda como var global\n",
    "   X_train = np.load(f)\n",
    "with open('X_test.npy', 'rb') as f:\n",
    "   X_test = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_train = np.load(r'./y_train.npy')\n",
    "Y_test = np.load(r'./y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = X_train[:1000]\n",
    "train_y = Y_train[:1000]\n",
    "test_x = X_test[:500]\n",
    "test_y = Y_test[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in range(5):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_x[i], cmap=plt.cm.binary)\n",
    "    # The CIFAR labels happen to be arrays, \n",
    "    # which is why you need the extra index\n",
    "    plt.xlabel(train_y[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "A diferencia del laboratorio realizado, este no es un problema de clasificación, sino de regresión, ya que queremos predecir la edad ósea en meses, una variable cuantitativa y no categórica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, kernel_size=(3, 3), activation='relu',input_shape=(img_size, img_size,1)))\n",
    "model.add(Conv2D(32, (5, 5), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',optimizer='adam',metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 140s 4s/step - loss: 17367.4844 - mse: 17367.4844 - val_loss: 18284.7207 - val_mse: 18284.7207\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 134s 4s/step - loss: 17367.4844 - mse: 17367.4844 - val_loss: 18284.7207 - val_mse: 18284.7207\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 135s 4s/step - loss: 17367.4844 - mse: 17367.4844 - val_loss: 18284.7207 - val_mse: 18284.7207\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 143s 4s/step - loss: 17367.4844 - mse: 17367.4844 - val_loss: 18284.7207 - val_mse: 18284.7207\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 139s 4s/step - loss: 17367.4844 - mse: 17367.4844 - val_loss: 18284.7207 - val_mse: 18284.7207\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 144s 4s/step - loss: 17367.4844 - mse: 17367.4844 - val_loss: 18284.7207 - val_mse: 18284.7207\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 140s 4s/step - loss: 17367.4844 - mse: 17367.4844 - val_loss: 18284.7207 - val_mse: 18284.7207\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 140s 4s/step - loss: 17367.4844 - mse: 17367.4844 - val_loss: 18284.7207 - val_mse: 18284.7207\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 140s 4s/step - loss: 17367.4844 - mse: 17367.4844 - val_loss: 18284.7207 - val_mse: 18284.7207\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 140s 4s/step - loss: 17367.4844 - mse: 17367.4844 - val_loss: 18284.7207 - val_mse: 18284.7207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x11caedba8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, epochs=10, validation_data=(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 14s 863ms/step - loss: 18284.7207 - mse: 18284.7207\n",
      "\n",
      "MSE: 18284.72\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_x, test_y)\n",
    "print(\"\\nMSE: %.2f\" % (results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('modelosimple.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_og = Sequential()\n",
    "\n",
    "model_og.add(Conv2D(16, kernel_size=(5, 5), activation='relu',input_shape=(img_size, img_size,1)))\n",
    "model_og.add(Dropout(rate = 0.25))\n",
    "\n",
    "model_og.add(Conv2D(32, (5, 5), activation='relu'))\n",
    "model_og.add(Dropout(rate = 0.25))\n",
    "model_og.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_og.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model_og.add(Dropout(rate = 0.25))\n",
    "model_og.add(Flatten())\n",
    "model_og.add(Dense(256, activation='relu'))\n",
    "model_og.add(Flatten())\n",
    "model_og.add(Dense(1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('modelosimple.h5')\n",
    "model_og = load_model('modelo_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 19s 1s/step - loss: 18027.9434 - accuracy: 0.0000e+00 - mean_squared_error: 18027.9434\n",
      "\n",
      "MSE: 18027.94\n"
     ]
    }
   ],
   "source": [
    "results_og = model_og.evaluate(test_x, test_y)\n",
    "print(\"\\nMSE: %.2f\" % (results_og[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "32/32 [==============================] - 139s 4s/step - loss: 17367.4844 - mean_squared_error: 17367.4844 - val_loss: 18284.7207 - val_mean_squared_error: 18284.7207\n",
      "Epoch 2/5\n",
      "32/32 [==============================] - 146s 5s/step - loss: 17367.4824 - mean_squared_error: 17367.4824 - val_loss: 18284.7207 - val_mean_squared_error: 18284.7207\n",
      "Epoch 3/5\n",
      "32/32 [==============================] - 145s 5s/step - loss: 17367.4844 - mean_squared_error: 17367.4844 - val_loss: 18284.7207 - val_mean_squared_error: 18284.7207\n",
      "Epoch 4/5\n",
      "32/32 [==============================] - 154s 5s/step - loss: 17367.4844 - mean_squared_error: 17367.4844 - val_loss: 18284.7207 - val_mean_squared_error: 18284.7207\n",
      "Epoch 5/5\n",
      "32/32 [==============================] - 147s 5s/step - loss: 17367.4844 - mean_squared_error: 17367.4844 - val_loss: 18284.7207 - val_mean_squared_error: 18284.7207\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "history = model.fit(train_x, train_y, validation_data=(test_x, test_y), epochs=5)\n",
    "plt.plot(history.history['mean_squared_error'])\n",
    "plt.plot(history.history['val_mean_squared_error'])\n",
    "plt.title('model mse')\n",
    "plt.ylabel('mse')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "32/32 [==============================] - 945s 30s/step - loss: 17117.6953 - accuracy: 0.0000e+00 - mean_squared_error: 17117.6953 - val_loss: 18027.9434 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 18027.9434\n",
      "Epoch 2/5\n",
      "32/32 [==============================] - 2248s 70s/step - loss: 17117.6953 - accuracy: 0.0000e+00 - mean_squared_error: 17117.6953 - val_loss: 18027.9434 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 18027.9434\n",
      "Epoch 3/5\n",
      "32/32 [==============================] - 981s 31s/step - loss: 17117.6953 - accuracy: 0.0000e+00 - mean_squared_error: 17117.6953 - val_loss: 18027.9434 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 18027.9434\n",
      "Epoch 4/5\n",
      "32/32 [==============================] - 1190s 37s/step - loss: 17117.6953 - accuracy: 0.0000e+00 - mean_squared_error: 17117.6953 - val_loss: 18027.9434 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 18027.9434\n",
      "Epoch 5/5\n",
      "32/32 [==============================] - 973s 30s/step - loss: 17117.6953 - accuracy: 0.0000e+00 - mean_squared_error: 17117.6953 - val_loss: 18027.9434 - val_accuracy: 0.0000e+00 - val_mean_squared_error: 18027.9434\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "history = model_og.fit(train_x, train_y, validation_data=(test_x, test_y), epochs=5)\n",
    "plt.plot(history.history['mean_squared_error'])\n",
    "plt.plot(history.history['val_mean_squared_error'])\n",
    "plt.title('model mse')\n",
    "plt.ylabel('mse')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hallazgos y conclusiones entrega #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta primera entrega nos enfocamos en comprender el dataset y planear como lo ibamos a manegar e interpretar para sacar resultados, al final decimos crear unas redes neuronales que comprendiar el dataset, para esto nos inspiramos en otras redes neuronales en internet que hacian cosas parecidas, pero a partir de este paso empezamos a tener algunas complicaciones que se detallaran en breve. \n",
    "\n",
    "Cuando pusimos a correr los modelos con los datos ya entrenados y testeados pero se obtuvieron resultados mixtos, ya que muchas complicaciones se nos presentaron, por ejemplo el dataset que se uso era muy pesado lo que provoco que la mayoria de las computdoras del grupo no pudieran prosesarlo a la hora de correr el modelo, por lo que intentamos usar una menor cantidad de datos para hacerlo mas ligero pero esto tambien nos dio malos resultados porque el accuracy que nos devolvia era de 0.00% ya que la cantidad de datos no era suficiente para poder tener mas acurracy, por lo que tampoco pudimos quedarnos con estos resultados, al final lo que se tuvo que hacer fue tomar una computadora mas potente la cual si pudo procesarlo aunque igual hay que recalcar que le llevo 10 minutos hacerlo.\n",
    "\n",
    "Después intentamos usar una red neuronal más simple la cual saco resultados parecidos a la compleja, por lo que para el resto del proyecto estaremos usando esta, de igual manera para tener mejores resultados seria mejor usar la compleja pero tomar en cuanta que esto solo si se tiene una computadora con un nivel alto de procesamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "1. Gilsanz, V. y Ratib, O. (2005). _Hand Bone Age_. Berlín: Springer.\n",
    "2. Dallora, A. L., Anderberg, P., Kvist, O., Mendes, E., Diaz Ruiz, S. y Berglund, J. S. (2019). Bone age assessment with various machine learning techniques: A systematic literature review and meta-analysis. _PLoS ONE_, 14(7): e0220242. Disponible en: https://doi.org/10.1371/journal.pone.0220242\n",
    "3. Husseini, H., Spronk, T., Masanneck, L. y Gutewort, L. (2019). Bone age prediction through x-ray images. _Medium_. Recuperado de https://medium.com/techlabsms/bone-age-prediction-through-x-ray-images-6e181d900a7a\n",
    "4. Navarro, M. M., Tejedor, B. M. y López Siguero, J. P. (2014) “El uso de la edad ósea en la práctica clínica”, _Anales de Pediatría Continuada_, 12(6), pp. 275–283. doi: 10.1016/s1696-2818(14)70204-5. Disponible en: https://www.elsevier.es/es-revista-anales-pediatria-continuada-51-articulo-el-uso-edad-osea-practica-S1696281814702045"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
